{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as manimation\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers.replay_buffer import ReplayBuffer\n",
    "from helpers.chain_environment import SimpleChain\n",
    "from helpers.shedules import LinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_var(arr, astype='float32', add_dim=False):\n",
    "    if add_dim:\n",
    "        v = Variable(torch.from_numpy(np.array([arr]).astype(astype)))\n",
    "    else:\n",
    "        v = Variable(torch.from_numpy(arr.astype(astype))) \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNnet(nn.Module):\n",
    "    def __init__(self, num_actions, input_dim, hidden_size=20):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                            nn.Linear(input_dim, hidden_size),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(hidden_size, num_actions))\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_model_copying():\n",
    "    # define models\n",
    "    model = DQNnet(num_actions=num_actions, input_dim=input_dim)\n",
    "    target_model = copy.deepcopy(model)\n",
    "    # create input variable\n",
    "    s = chain_env.reset()\n",
    "    s_var = convert_to_var(s, add_dim=True)\n",
    "    # eval outputs of models on this variable\n",
    "    model_output = model.forward(s_var)\n",
    "    target_model_output = target_model.forward(s_var).detach() # detach stop gradients\n",
    "    # check that both modules give the same outputs\n",
    "    np.testing.assert_array_equal(model_output.data.numpy(), target_model_output.data.numpy())\n",
    "    # define everything necessary for optimization step and make opt step\n",
    "    mse_loss_func = nn.MSELoss()\n",
    "    loss = mse_loss_func(model_output, target_model_output+10)\n",
    "    optimizer = torch.optim.RMSprop(model.parameters())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # eval outputs of models on the variable after opt step\n",
    "    upd_model_output = model.forward(s_var)\n",
    "    upd_target_model_output = target_model.forward(s_var).detach()\n",
    "    # check that parameters of model have been updated\n",
    "    assert (model_output.data.numpy() == upd_model_output.data.numpy).sum() == 0\n",
    "    # check that parameters of target model have not been updated\n",
    "    np.testing.assert_array_equal(upd_target_model_output.data.numpy(), target_model_output.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_dqn_loss(optimizer, model, target_model, batch, gamma,\n",
    "                      target_type='standard', tau=None):\n",
    "    states_batch, actions_batch, rewards_batch, next_states_batch, dones_batch = batch\n",
    "    states_batch_var = convert_to_var(states_batch)\n",
    "    actions_batch_var = convert_to_var(actions_batch[:, np.newaxis], astype='int64')\n",
    "    rewards_batch_var = convert_to_var(rewards_batch)\n",
    "    next_states_batch_var = convert_to_var(next_states_batch)\n",
    "    dones_batch_var = convert_to_var(dones_batch)\n",
    "    \n",
    "    q_values = model.forward(states_batch_var).gather(1, actions_batch_var)\n",
    "\n",
    "    next_q_values = target_model.forward(next_states_batch_var).detach()\n",
    "    next_q_values = next_q_values.max(dim=1)[0]\n",
    "    next_q_values[dones_batch_var.byte()] = 0\n",
    "    q_values_targets = rewards_batch_var + gamma * next_q_values\n",
    "\n",
    "    mse_loss_func = nn.MSELoss()\n",
    "    loss = mse_loss_func(q_values, q_values_targets)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "def clear_folder(folder):\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_amd_log_images(state_action_count, all_q_values, t, folder):\n",
    "    if t == 0:\n",
    "        clear_folder(folder)\n",
    "        \n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(121)\n",
    "    s_a_visitations_plot = plt.imshow(state_action_count / state_action_count.sum(), vmin=0, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.title('S-A Visitations')\n",
    "    plt.subplot(122)\n",
    "    q_values_plot = plt.imshow(all_q_values)\n",
    "    plt.colorbar()\n",
    "    plt.title('Q values')\n",
    "    plt.savefig(folder+'/{}.png'.format(t))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_agent(env, model, agent_type='simple_dqn'):\n",
    "    episode_total_reward = 0\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = epsilon_greedy_act(state, model, 0)\n",
    "        next_state, rew, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        episode_total_reward += rew\n",
    "        if done:\n",
    "            break\n",
    "    env.reset()\n",
    "    return episode_total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_act(state, model, eps_t):\n",
    "    state_var = convert_to_var(state, add_dim=True)\n",
    "    q_values = model.forward(state_var).data.numpy()[0]\n",
    "    if np.random.rand() < eps_t:\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        action = q_values.argmax()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "def train(env,\n",
    "          eps_greedy_exploration_params,\n",
    "          gamma=0.99,\n",
    "          max_steps=100,\n",
    "          learning_starts_in_steps=100,\n",
    "          train_freq_in_steps=1,\n",
    "          update_freq_in_steps=10,\n",
    "          plot_freq_in_steps=10,\n",
    "          eval_freq_in_episodes=5,\n",
    "          seed=None\n",
    "          ):\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    num_actions = env.action_space.n\n",
    "    dim_states = env.observation_space.shape[0]\n",
    "    n_all_states = env.get_all_states().shape[0]\n",
    "    # define models\n",
    "    model = DQNnet(num_actions, dim_states)\n",
    "    target_model = copy.deepcopy(model)\n",
    "    # define optimizator\n",
    "    optimizer = torch.optim.RMSprop(model.parameters())\n",
    "    # define shedule of epsilon in epsilon-greedy exploration\n",
    "    schedule_timesteps=int(eps_greedy_exploration_params['exploration_fraction'] * max_steps)\n",
    "    eps_shedule = LinearSchedule(schedule_timesteps=schedule_timesteps,\n",
    "                                 initial_p=1.0,\n",
    "                                 final_p=eps_greedy_exploration_params['exploration_final_eps'])\n",
    "    folder = 'mylogs'\n",
    "    clear_folder(folder)\n",
    "    writer = SummaryWriter(folder)\n",
    "    \n",
    "    replay_buffer = ReplayBuffer(1000, seed=seed)\n",
    "    num_episodes = 0\n",
    "    sum_rewards_per_episode = [0]\n",
    "    list_rewards_per_episode = [[]]\n",
    "    state_action_count = np.zeros((n_all_states, num_actions))\n",
    "    count_good_rewards = 0\n",
    "    state = env.reset()\n",
    "    for t in range(max_steps):\n",
    "        eps_t = eps_shedule.value(t)\n",
    "        action = epsilon_greedy_act(state, model, eps_t)\n",
    "        state_action_count[env.cur_state_id][action] += 1\n",
    "\n",
    "        next_state, rew, done, _ = env.step(action)\n",
    "        replay_buffer.add(state, action, rew, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        if rew == 1:\n",
    "            count_good_rewards += 1\n",
    "        sum_rewards_per_episode[-1] += rew\n",
    "        list_rewards_per_episode[-1].append(rew)\n",
    "        if done:\n",
    "            num_episodes += 1\n",
    "            print('Episodes:', num_episodes, sum_rewards_per_episode[-1])\n",
    "            sum_rewards_per_episode.append(0)\n",
    "            list_rewards_per_episode.append([])\n",
    "            state = env.reset()\n",
    "\n",
    "        if t > learning_starts_in_steps and t % train_freq_in_steps == 0:\n",
    "            batch = replay_buffer.sample(batch_size)\n",
    "            loss = optimize_dqn_loss(optimizer, model, target_model, batch, gamma)\n",
    "            writer.add_scalar('dqn/loss', loss, t)\n",
    "\n",
    "        if t > learning_starts_in_steps and t % update_freq_in_steps == 0:\n",
    "            target_model = copy.deepcopy(model)\n",
    "        \"\"\"    \n",
    "        if done and eval_freq_in_episodes is not None and num_episodes % eval_freq_in_episodes == 0:\n",
    "            test_episode_reward = eval_agent(env, model)\n",
    "            if test_episode_reward == 10:\n",
    "                print('Successfully solved environment in {} episodes'.format(num_episodes))\n",
    "                break\n",
    "        \"\"\"\n",
    "\n",
    "        all_states_var = convert_to_var(env.get_all_states())\n",
    "        all_q_values = model.forward(all_states_var).data.numpy()\n",
    "        for i in range(n_all_states):\n",
    "            if (2 <= i < n_all_states - 2) and n_all_states > 10:\n",
    "                continue\n",
    "            else:\n",
    "                writer.add_scalars('dqn/q_values/state_{}'.format(i+1), {'action_right': all_q_values[i][1],\n",
    "                                                                         'action_left': all_q_values[i][0]},\n",
    "                                                                      t)\n",
    "        writer.add_scalar('dqn/count_good_reward', count_good_rewards, t)\n",
    "        writer.add_scalar('dqn/eps_t', eps_t, t)\n",
    "        \n",
    "        if t % plot_freq_in_steps == 0:\n",
    "            plot_amd_log_images(state_action_count, all_q_values, t, 'logs/images_logs/images')\n",
    "    return state_action_count, num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleChain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-76101b4793a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mchain_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSimpleChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnum_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdim_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SimpleChain' is not defined"
     ]
    }
   ],
   "source": [
    "input_dim=5\n",
    "chain_env=SimpleChain(input_dim)\n",
    "num_actions = chain_env.action_space.n\n",
    "dim_states = chain_env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eps_greedy_exploration_params = {'exploration_fraction': 0.5,\n",
    "                                 'exploration_final_eps': 0.05,\n",
    "                                 'flag': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chain_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7c55fd5953eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m state_action_count, num_episodes = train(chain_env,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                           \u001b[0meps_greedy_exploration_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                           \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                           \u001b[0mlearning_starts_in_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                           \u001b[0mupdate_freq_in_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chain_env' is not defined"
     ]
    }
   ],
   "source": [
    "state_action_count, num_episodes = train(chain_env,\n",
    "                                          eps_greedy_exploration_params,\n",
    "                                          max_steps=500*(input_dim+9),\n",
    "                                          learning_starts_in_steps=5*(input_dim+9),\n",
    "                                          update_freq_in_steps=1*(input_dim+9),\n",
    "                                          plot_freq_in_steps=1*(input_dim+9),\n",
    "                                          seed=12\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_action_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_action_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
